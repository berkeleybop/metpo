{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "classify_temp_strings.py\n",
    "────────────────────────\n",
    "usage:\n",
    "    python classify_temp_strings.py  In.csv  Out.csv\n",
    "\n",
    "The input CSV must contain at least a column called ‘object’.\n",
    "Ten Boolean columns (has_number … has_any_other_text) are added.\n",
    "\"\"\""
   ],
   "id": "312271d18d108ebb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from decimal import Decimal\n",
    "from typing import Callable, TypeVar\n",
    "from typing import Dict, Optional\n",
    "from typing import Iterable, Final, Pattern\n",
    "from typing import Union\n",
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdflib import (\n",
    "    Graph, Namespace, BNode, URIRef, Literal, RDF, XSD,\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import os"
   ],
   "id": "dcd9db1913105d6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "ABOVE_RX = re.compile(r\"\\babove\\b\", re.I)\n",
    "AND_RX = re.compile(r\"\\band\\b\", re.I)\n",
    "AT_RX = re.compile(r\"\\bat\\b\", re.I)\n",
    "BELOW_RX = re.compile(r\"\\bbelow\\b\", re.I)\n",
    "CELSIUS_RX = re.compile(r\"(?<![A-Za-z])(?:c|℃)(?![A-Za-z])\", re.I)\n",
    "COMMA_RX = re.compile(r\",\")\n",
    "DEGREE_RX = re.compile(r\"[°º˚℃]\")\n",
    "DIGIT_RX = re.compile(r\"\\d\")\n",
    "FAHREN_RX = re.compile(r\"(?<![A-Za-z])(?:°\\s*F|f(?:ahrenheit)?)(?![A-Za-z])\", re.I)\n",
    "HYPHEN_RX = re.compile(r\"[-–—−]\")\n",
    "INEQ_RX = re.compile(r\"[<>≤≥⩽⩾]\")\n",
    "INITIAL_HYPHEN_RX = re.compile(r\"^\\s*[-–—−]\")\n",
    "INIT_ABOVE_RX = re.compile(r\"^\\s*above\\s*\", re.I)\n",
    "INIT_BELOW_RX = re.compile(r\"^\\s*below\\s*\", re.I)\n",
    "INTERNAL_ABOVE_RX = re.compile(r'^\\s*\\S+.*\\babove\\b', re.I)\n",
    "INTERNAL_BELOW_RX = re.compile(r'^\\s*\\S+.*\\bbelow\\b', re.I)\n",
    "KELVIN_RX = re.compile(r\"(?<![A-Za-z])(?:°\\s*K|kelvin|k)(?![A-Za-z])\", re.I)\n",
    "LETTER_RX = re.compile(r\"[A-Za-z]\")\n",
    "MIN_RX = re.compile(r\"\\bmin\\b\", re.I)\n",
    "OPHIL_RX = re.compile(r\"ophil\", re.I)\n",
    "OR_RX = re.compile(r\"\\bor\\b\", re.I)\n",
    "PAREN_RX = re.compile(r\"[()]\")\n",
    "PLUSMINUS_RX = re.compile(r\"±|\\+/?-\")\n",
    "POOR_RX = re.compile(r\"\\bpoor\\b\", re.I)\n",
    "QUALIFIER_RX = re.compile(r\"\\b(optimum|optimal|max(?:imum)?|min(?:imum)?|upper limit|lower limit)\\b\", re.I)\n",
    "RANGEWORD_RX = re.compile(r\"\\b(between|from|until)\\b|<\\s*\\d+\\s*<\", re.I)\n",
    "SLASH_RX = re.compile(r\"[\\/|]\")\n",
    "TERM_PUNCT_RX = re.compile(r\"[.;,]$\")\n",
    "TIME_RX = re.compile(r\"\\b(?:min(?:ute)?s?|h(?:ours?)?|hr?s?|sec(?:ond)?s?|days?)\\b\", re.I)\n",
    "TOLERAN_RX = re.compile(r\"toleran\", re.I)\n",
    "TO_RX = re.compile(r\"\\bto\\b\", re.I)\n",
    "UP_TO_RX = re.compile(r\"\\bup\\s*to\\b\", re.I)  # <── NEW\n",
    "WEAK_RX = re.compile(r\"\\b(weak|weakly)\\b\", re.I)\n",
    "WORD_DEGREE_RX = re.compile(r\"\\bdeg(?:ree)?s?\\s*(?:c|cel(?:sius)?)\\b\", re.I)\n"
   ],
   "id": "d957da4ee2018258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "#  Patterns whose matches should be removed before \"other text\" test\n",
    "# ------------------------------------------------------------------\n",
    "_STRIPPABLE_RXES: list[re.Pattern] = [\n",
    "    # HYPHEN_RX,\n",
    "    # INEQ_RX,\n",
    "    # INITIAL_HYPHEN_RX,\n",
    "    # PAREN_RX,\n",
    "    # PLUSMINUS_RX,\n",
    "    # SLASH_RX,\n",
    "    ABOVE_RX,\n",
    "    AND_RX,\n",
    "    AT_RX,\n",
    "    BELOW_RX,\n",
    "    CELSIUS_RX,\n",
    "    COMMA_RX,\n",
    "    DEGREE_RX,\n",
    "    FAHREN_RX,\n",
    "    KELVIN_RX,\n",
    "    MIN_RX,\n",
    "    OPHIL_RX,\n",
    "    OR_RX,\n",
    "    POOR_RX,\n",
    "    QUALIFIER_RX,\n",
    "    RANGEWORD_RX,\n",
    "    TIME_RX,\n",
    "    TOLERAN_RX,\n",
    "    TO_RX,\n",
    "    UP_TO_RX,\n",
    "    WEAK_RX,\n",
    "    WORD_DEGREE_RX,\n",
    "]"
   ],
   "id": "a2ee1b729333110f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# build one big strip-regex for the “other text” test\n",
    "STRIP_RX = re.compile(\"|\".join(rx.pattern for rx in _STRIPPABLE_RXES), re.I)"
   ],
   "id": "9679b2395f842508",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This works in Jupyter and with papermill --cwd\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir if (notebook_dir / \"Makefile\").exists() else notebook_dir.parent\n",
    "assets_dir = project_root / \"assets\"\n",
    "local_dir = project_root / \"local\""
   ],
   "id": "49581dff13fe5ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "in_path = local_dir / \"n4l-temperature.csv\"\n",
    "ttl_out = local_dir / \"n4l-temperature.ttl\"\n",
    "unparsed_out = local_dir / \"n4l-temperature-un-parsed.csv\""
   ],
   "id": "d88037c12bf93645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv(in_path, dtype=str).fillna(\"\")",
   "id": "cb341686931161f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"object\" not in df.columns:\n",
    "    sys.exit(\"Input must contain a column called ‘object’\")"
   ],
   "id": "39129f59f8fae572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "s = df[\"object\"]",
   "id": "141a82b62e84d856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------------------------\n",
    "#  3.  Vectorised feature checks\n",
    "# ----------------------------------------------------------------------\n",
    "tests = {\n",
    "    \"has_above\": ABOVE_RX,\n",
    "    \"has_and\": AND_RX,\n",
    "    \"has_any_hyphen\": HYPHEN_RX,\n",
    "    \"has_any_initial_hyphen\": INITIAL_HYPHEN_RX,\n",
    "    \"has_at\": AT_RX,\n",
    "    \"has_below\": BELOW_RX,\n",
    "    \"has_celsius\": CELSIUS_RX,\n",
    "    \"has_degree_symbol\": DEGREE_RX,\n",
    "    \"has_inequality_symbol\": INEQ_RX,\n",
    "    \"has_init_above\": INIT_ABOVE_RX,\n",
    "    \"has_init_below\": INIT_BELOW_RX,\n",
    "    \"has_internal_above\": INTERNAL_ABOVE_RX,\n",
    "    \"has_internal_below\": INTERNAL_BELOW_RX,\n",
    "    \"has_internal_comma\": re.compile(r\",(?=.)\"),  # comma not at end\n",
    "    \"has_number\": DIGIT_RX,\n",
    "    \"has_ophil\": OPHIL_RX,\n",
    "    \"has_or\": OR_RX,\n",
    "    \"has_parentheses\": PAREN_RX,\n",
    "    \"has_plus_minus\": PLUSMINUS_RX,\n",
    "    \"has_poor\": POOR_RX,\n",
    "    \"has_qualifier_word\": QUALIFIER_RX,\n",
    "    \"has_range_keyword\": RANGEWORD_RX,\n",
    "    \"has_slash_separator\": SLASH_RX,\n",
    "    \"has_terminal_punctuation\": TERM_PUNCT_RX,\n",
    "    \"has_time_expression\": TIME_RX,\n",
    "    \"has_to\": TO_RX,\n",
    "    \"has_toleran\": TOLERAN_RX,\n",
    "    \"has_unit_fahrenheit\": FAHREN_RX,\n",
    "    \"has_unit_kelvin\": KELVIN_RX,\n",
    "    \"has_up_to\": UP_TO_RX,\n",
    "    \"has_weak\": WEAK_RX,\n",
    "    \"has_word_degree\": WORD_DEGREE_RX,\n",
    "}"
   ],
   "id": "57bac62ba78d2f54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boolean DataFrame built in one comprehension\n",
    "flag_df = pd.DataFrame({\n",
    "    name: s.str.contains(rx, regex=True, na=False)\n",
    "    for name, rx in tests.items()\n",
    "})"
   ],
   "id": "2be892282f4b0017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------------------------\n",
    "#  4.  “Has any OTHER text”  (also vectorised)\n",
    "# ----------------------------------------------------------------------\n",
    "stripped = s.str.replace(STRIP_RX, \" \", regex=True)\n",
    "flag_df[\"has_any_other_text\"] = stripped.str.contains(LETTER_RX, na=False)"
   ],
   "id": "b48e8cc030bd3254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "enriched = pd.concat([df, flag_df], axis=1)\n",
    "\n",
    "enriched_len = enriched.shape[0]\n"
   ],
   "id": "35c152a8dc1e3db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "FLAGS = sorted(c for c in enriched.columns if c.startswith(\"has_\"))",
   "id": "567a8c0d10c9c014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NFLAGS = len(FLAGS)",
   "id": "b9e52d7fa4862b1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if NFLAGS > 63:\n",
    "    raise ValueError(\"bitmask needs uint64 – you have >63 flags\")"
   ],
   "id": "353416db807403bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "powers_of_two = 1 << np.arange(NFLAGS)",
   "id": "9dd6243fb97804be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "enriched[\"pattern_id\"] = (\n",
    "    enriched[FLAGS].astype(np.uint64).dot(powers_of_two).astype(np.uint64)\n",
    ")"
   ],
   "id": "566261647860aaa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "flag2bit = {f: i for i, f in enumerate(FLAGS)}",
   "id": "8e4f7d1560f30835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bit2flag = {i: f for f, i in flag2bit.items()}",
   "id": "133280fdcf61bb0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1.  ↔ transformation helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def flags_to_pid(flag_names):\n",
    "    \"\"\"\n",
    "    Convert an iterable of flag names to one integer pattern_id.\n",
    "    \"\"\"\n",
    "    missing = set(flag_names) - flag2bit.keys()\n",
    "    if missing:\n",
    "        raise ValueError(f\"Unknown flag name(s): {', '.join(missing)}\")\n",
    "\n",
    "    pid = 0\n",
    "    for name in flag_names:\n",
    "        pid |= 1 << flag2bit[name]\n",
    "    return pid"
   ],
   "id": "c8f405e766f7709d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pid_to_flags(pid):\n",
    "    \"\"\"\n",
    "    Return the list of *true* flag names for this pattern_id.\n",
    "    \"\"\"\n",
    "    return [bit2flag[i] for i in range(NFLAGS) if pid & (1 << i)]"
   ],
   "id": "fbc65181f4986ed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 2.  row-selection helpers  (with optional false_flags)\n",
    "# ---------------------------------------------------------------------\n",
    "def _validate_flags(df, flags):\n",
    "    \"\"\"Raise if any flag name is unknown.\"\"\"\n",
    "    missing = set(flags) - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Unknown flag name(s): {', '.join(missing)}\")"
   ],
   "id": "a7691a9f7b69009b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def select_exact(df, true_flags, *, false_flags=None, flags_col_prefix=\"has_\"):\n",
    "    \"\"\"\n",
    "    Keep rows where\n",
    "        • every flag in true_flags  is  True\n",
    "        • every flag in false_flags (if given) is False\n",
    "        • all remaining has_* columns are also False\n",
    "    \"\"\"\n",
    "    true_flags = list(true_flags)\n",
    "    false_flags = list(false_flags or [])\n",
    "    _validate_flags(df, true_flags + false_flags)\n",
    "\n",
    "    # True constraints\n",
    "    mask = df[true_flags].all(axis=1)\n",
    "\n",
    "    # Explicit False constraints (if any)\n",
    "    if false_flags:\n",
    "        mask &= (~df[false_flags]).all(axis=1)\n",
    "\n",
    "    # For 'exact' we require all *other* has_* columns to be False\n",
    "    other_flags = [\n",
    "        c for c in df.columns\n",
    "        if c.startswith(flags_col_prefix) and c not in true_flags + false_flags\n",
    "    ]\n",
    "    mask &= (~df[other_flags]).all(axis=1)\n",
    "\n",
    "    return df[mask]"
   ],
   "id": "c02bfe4f5b8ce351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def select_at_least(df, true_flags, *, false_flags=None):\n",
    "    \"\"\"\n",
    "    Keep rows where\n",
    "        • every flag in true_flags  is True\n",
    "        • every flag in false_flags (if given) is False\n",
    "        • the rest may be either\n",
    "    \"\"\"\n",
    "    true_flags = list(true_flags)\n",
    "    false_flags = list(false_flags or [])\n",
    "    _validate_flags(df, true_flags + false_flags)\n",
    "\n",
    "    mask = df[true_flags].all(axis=1)\n",
    "    if false_flags:\n",
    "        mask &= (~df[false_flags]).all(axis=1)\n",
    "\n",
    "    return df[mask]"
   ],
   "id": "267715ee58c0c8c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "befaa9114006d929"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def most_frequent_pattern_id(df: pd.DataFrame, rank: int = 1) -> tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Return (pattern_id, frequency) for the *rank*-th most common pattern_id\n",
    "    in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame\n",
    "           Must contain a column called 'pattern_id'.\n",
    "    rank : int, default 1\n",
    "           1 → most common, 2 → second most common, …\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError  if rank < 1 or rank exceeds the number of distinct patterns.\n",
    "    \"\"\"\n",
    "    if rank < 1:\n",
    "        raise ValueError(\"rank must be ≥ 1\")\n",
    "\n",
    "    vc = df[\"pattern_id\"].value_counts()\n",
    "\n",
    "    if rank > len(vc):\n",
    "        raise ValueError(\n",
    "            f\"rank={rank} exceeds the number of distinct patterns ({len(vc)})\"\n",
    "        )\n",
    "\n",
    "    pid = int(vc.index[rank - 1])  # pattern_id at the requested rank\n",
    "    freq = int(vc.iloc[rank - 1])  # its frequency\n",
    "\n",
    "    return pid, freq, rank"
   ],
   "id": "a0a22257a4e8e230",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------\n",
    "#  new helper: summary up to a given rank\n",
    "# ------------------------------------------------------------\n",
    "def top_pattern_summary(df: pd.DataFrame, top_n: int = 10,\n",
    "                        pattern_col: str = \"pattern_id\") -> OrderedDict:\n",
    "    \"\"\"\n",
    "    Return an OrderedDict\n",
    "        rank  →  {\"pattern_id\": int,\n",
    "                  \"frequency\": int,\n",
    "                  \"true_flags\": list[str]}\n",
    "    for the `top_n` most frequent patterns in *df*.\n",
    "    \"\"\"\n",
    "    if pattern_col not in df.columns:\n",
    "        raise KeyError(f\"Column {pattern_col!r} not found\")\n",
    "\n",
    "    vc = df[pattern_col].value_counts()\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    for rank, (pid, freq) in enumerate(vc.items(), 1):\n",
    "        if rank > top_n:\n",
    "            break\n",
    "        summary[rank] = {\n",
    "            \"pattern_id\": int(pid),\n",
    "            \"count\": int(freq),\n",
    "            \"true_flags\": pid_to_flags(int(pid)),\n",
    "        }\n",
    "    return summary"
   ],
   "id": "76e1559cb87b87a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def value_counts_df(\n",
    "        df: pd.DataFrame,\n",
    "        column: Union[str, int],\n",
    "        normalize: bool = False,\n",
    "        sort_desc: bool = True,\n",
    "        dropna: bool = True,\n",
    "        name_value: str = \"value\",\n",
    "        name_count: str = \"count\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a two-column DataFrame that contains the value-counts of *column*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df        : DataFrame\n",
    "    column    : str | int\n",
    "                The column whose value distribution you want.\n",
    "    normalize : bool, default False\n",
    "                If True, return relative frequencies (0–1) instead of counts.\n",
    "    sort_desc : bool, default True\n",
    "                Sort by frequency descending (True) or ascending (False).\n",
    "    dropna    : bool, default True\n",
    "                If False, include NaN/None as a separate category.\n",
    "    name_value: str, default \"value\"\n",
    "    name_count: str, default \"count\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame  with columns [name_value, name_count].\n",
    "    \"\"\"\n",
    "    vc = df[column].value_counts(\n",
    "        normalize=normalize,\n",
    "        dropna=dropna,\n",
    "        ascending=not sort_desc\n",
    "    )\n",
    "\n",
    "    out = vc.rename_axis(name_value).reset_index(name=name_count)\n",
    "    return out"
   ],
   "id": "7dd13a4da09023d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "summary = top_pattern_summary(enriched, top_n=30)",
   "id": "2eb3aa6a4d37813c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "summary",
   "id": "ebdcf37af8e67e22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "begin here",
   "id": "5ece937ab4dbdbb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "requested_pid, pid_count, pid_rank = most_frequent_pattern_id(enriched, rank=14)\n",
    "true_flags = pid_to_flags(requested_pid)\n",
    "subset_exact = select_exact(enriched, true_flags)\n",
    "positive_object_values = value_counts_df(\n",
    "    subset_exact,\n",
    "    \"object\",\n",
    ")"
   ],
   "id": "126cf27bf5588bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"pattern {requested_pid} has rank {pid_rank} with {pid_count} rows\")",
   "id": "e893309c5e6090c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "true_flags",
   "id": "b72fc429f543d32e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "positive_object_values",
   "id": "dc1048a937cbca6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================================\n",
    "# 1.  RANGE-ONLY temperature parser  ---------------------------------------\n",
    "# ==========================================================================\n",
    "_UNIT_TABLE = [\n",
    "    (r'(?:°|º)?\\s*C(?:elsius)?', 'Cel'),\n",
    "    (r'(?:°|º)?\\s*F', '[degF]'),\n",
    "    (r'\\bK\\b', 'K'),\n",
    "]\n",
    "\n",
    "_DASHES = \"-–—‒−‐\"\n",
    "_DASH_TRANS = str.maketrans({d: \"-\" for d in _DASHES})\n",
    "_NUMBER_RGX = re.compile(r'(?<!\\d)[+-]?\\d+(?:\\.\\d+)?')\n",
    "_RANGE_TOKEN_RGX = re.compile(r'-|\\bto\\b', re.I)\n",
    "_UNIT_STRIP_RGX = re.compile(r'(?:°|º)?\\s*[CFK](?:elsius)?', flags=re.I)\n",
    "\n",
    "\n",
    "def parse_hyphen_range_celsius_degree_values(raw: str) -> Dict:\n",
    "    comp: Dict = {\"component_text\": raw}\n",
    "\n",
    "    txt = (raw.translate(_DASH_TRANS)\n",
    "           .replace(\"\\u2009\", \" \")\n",
    "           .replace(\"\\u202f\", \" \"))\n",
    "\n",
    "    # unit\n",
    "    for pat, ucum in _UNIT_TABLE:\n",
    "        if re.search(pat, txt, flags=re.I):\n",
    "            comp[\"unit\"] = ucum\n",
    "            break\n",
    "\n",
    "    txt_unitless = _UNIT_STRIP_RGX.sub(\"\", txt)\n",
    "\n",
    "    if _RANGE_TOKEN_RGX.search(txt_unitless):\n",
    "        nums = [Decimal(n) for n in _NUMBER_RGX.findall(txt_unitless)]\n",
    "        if len(nums) >= 2:\n",
    "            comp[\"minimum_value\"], comp[\"maximum_value\"] = nums[:2]\n",
    "            if comp[\"minimum_value\"] > comp[\"maximum_value\"]:\n",
    "                comp[\"minimum_value\"], comp[\"maximum_value\"] = (\n",
    "                    comp[\"maximum_value\"], comp[\"minimum_value\"])\n",
    "            return comp  # SUCCESS → return with range fields set\n",
    "\n",
    "    # NOT a clear range\n",
    "    comp[\"unparsed_text\"] = raw\n",
    "    return comp"
   ],
   "id": "8e3fecbc9f7cf18d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# 1.  PARSER  (spot value in °C / °F / K)   ----------------------------------\n",
    "# ============================================================================\n",
    "\n",
    "_UNIT_TABLE = [\n",
    "    (r'(?:°|º)?\\s*C(?:elsius)?', 'Cel'),\n",
    "    (r'(?:°|º)?\\s*F', '[degF]'),\n",
    "    (r'\\bK\\b', 'K'),\n",
    "]\n",
    "\n",
    "_NUMBER_RGX = re.compile(r'(?<!\\d)[+-]?\\d+(?:\\.\\d+)?')  # prevents “25-30”→“-30”\n",
    "\n",
    "\n",
    "def parse_spot_celsius_degree_value(raw: str) -> Dict:\n",
    "    \"\"\"Return a dict that meets the ParseComponent cardinality rule.\"\"\"\n",
    "    comp: Dict = {\"component_text\": raw}\n",
    "\n",
    "    if m := _NUMBER_RGX.search(raw):\n",
    "        comp[\"spot_value\"] = Decimal(m.group())\n",
    "\n",
    "    for pat, ucum in _UNIT_TABLE:\n",
    "        if re.search(pat, raw, flags=re.I):\n",
    "            comp[\"unit\"] = ucum\n",
    "            break\n",
    "\n",
    "    if \"spot_value\" not in comp or \"unit\" not in comp:\n",
    "        comp[\"unparsed_text\"] = raw\n",
    "\n",
    "    return comp"
   ],
   "id": "d990600719dbb08f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<https://w3id.org/biolink/vocab/has_phenotype> values from KG-Microbe\n",
    "\n",
    "* https://www.example.org/UNKNOWN/temperature:hyperthermophilic\t\"263\"^^xsd:integer\n",
    "* https://www.example.org/UNKNOWN/temperature:mesophilic\t\"26633\"^^xsd:integer\n",
    "* https://www.example.org/UNKNOWN/temperature:psychrophilic\t\"1406\"^^xsd:integer\n",
    "* https://www.example.org/UNKNOWN/temperature:thermophilic\t\"1942\"^^xsd:integer"
   ],
   "id": "2ca49e7999c9bfff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================================\n",
    "# 1.  Canonical categorical & qualifier vocabulary  ------------------------\n",
    "#    – keys are canonical tokens we want in RDF\n",
    "#    – each value is ONE regex that matches plural, singular, adjective …\n",
    "# ==========================================================================\n",
    "_CATEGORIES = {\n",
    "    # canonical        regex that matches ALL listed variants\n",
    "    \"hyperthermophilic\": r\"\\bhyperthermophil(?:e|es|ic|ics)?\\b\",\n",
    "    \"mesophilic\": r\"\\bmesophil(?:e|es|ic|ics|ism|s)?\\b\",\n",
    "    \"psychrophilic\": r\"\\bpsychrophil(?:e|es|ic|ics|s)?\\b\",\n",
    "    \"psychrotolerant\": r\"\\bpsychrotolerant(?:s)?\\b\",\n",
    "    \"psychrotrophic\": r\"\\bpsychrotroph(?:e|es|ic|ics|s)?\\b\",\n",
    "    \"thermoacidophilic\": r\"\\bthermoacidophil(?:e|es|ic|ics)?\\b\",\n",
    "    \"thermophilic\": r\"\\b(?:thermophil(?:e|es|ic|ics|ism|s)|thermophile)\\b\",\n",
    "}\n",
    "\n",
    "_QUALIFIERS = {\n",
    "    \"moderately\": r\"\\bmoderate(?:ly)?\\b\",\n",
    "    \"slightly\": r\"\\bslight(?:ly)?\\b\",\n",
    "    \"extremely\": r\"\\bextreme(?:ly)?\\b\",\n",
    "    \"strict\": r\"\\bstrict(?:ly)?\\b\",\n",
    "    \"obligate\": r\"\\bobligat(?:e|ely)\\b\",\n",
    "    # add more if you encounter them\n",
    "}\n",
    "\n",
    "# pre-compile once for speed\n",
    "_CAT_RGX = {canon: re.compile(rx, re.I) for canon, rx in _CATEGORIES.items()}\n",
    "_QUAL_RGX = {canon: re.compile(rx, re.I) for canon, rx in _QUALIFIERS.items()}\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# 2.  The parser function  --------------------------------------------------\n",
    "# ==========================================================================\n",
    "def parse_categorical_label(raw: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse ONE free-text string for a categorical_label (mesophilic,\n",
    "    thermophilic, …) and an optional qualifier_label (moderately, very, …).\n",
    "\n",
    "    Output dict satisfies the ParseComponent cardinality rule:\n",
    "      • contains categorical_label   → OK\n",
    "      • **or** sets unparsed_text    → OK\n",
    "    \"\"\"\n",
    "    comp: Dict = {\"component_text\": raw}\n",
    "\n",
    "    # ------------------- detect category ----------------------------------\n",
    "    for canon, rgx in _CAT_RGX.items():\n",
    "        if rgx.search(raw):\n",
    "            comp[\"categorical_label\"] = canon\n",
    "            break  # stop at first hit\n",
    "\n",
    "    # ------------------- detect qualifier (optional) ----------------------\n",
    "    for canon, rgx in _QUAL_RGX.items():\n",
    "        if rgx.search(raw):\n",
    "            comp[\"qualifier_label\"] = canon\n",
    "            break\n",
    "\n",
    "    # ------------------- fall-back ----------------------------------------\n",
    "    if \"categorical_label\" not in comp:\n",
    "        comp[\"unparsed_text\"] = raw\n",
    "\n",
    "    return comp\n",
    "\n"
   ],
   "id": "8e279d26cf15f97a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1.  pre-compiled regexes\n",
    "# -------------------------------------------------------------------------\n",
    "_RANGE_TO_RGX = re.compile(\n",
    "    r\"\"\"\n",
    "    (?P<min>[+-]?\\d+(?:\\.\\d+)?)     # first number\n",
    "    \\s*(?:°|º)?\\s*C?                # optional °,º,C after first number\n",
    "    \\s*to\\s*                        # the word 'to' (any case)\n",
    "    (?P<max>[+-]?\\d+(?:\\.\\d+)?)     # second number\n",
    "    \\s*(?:°|º)?\\s*C?                # optional °,º,C after second number\n",
    "    \"\"\",\n",
    "    flags=re.I | re.X,  # ignore case + verbose mode\n",
    ")\n",
    "\n",
    "_UNIT_PRESENT_RGX = re.compile(r'(?:°|º)|\\bC\\b', flags=re.I)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2.  parser function\n",
    "# -------------------------------------------------------------------------\n",
    "def parse_to_temperature_range(raw: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse strings like\n",
    "        \"10 to 45\"\n",
    "        \"10° C to 45° C\"\n",
    "        \"10°to45C\"\n",
    "    into {minimum_value, maximum_value, unit?} or fall back to unparsed_text.\n",
    "    \"\"\"\n",
    "    comp: Dict = {\"component_text\": raw}\n",
    "\n",
    "    if m := _RANGE_TO_RGX.search(raw):\n",
    "        comp[\"minimum_value\"] = Decimal(m.group(\"min\"))\n",
    "        comp[\"maximum_value\"] = Decimal(m.group(\"max\"))\n",
    "\n",
    "        # swap if entered in descending order (rare but safe)\n",
    "        if comp[\"minimum_value\"] > comp[\"maximum_value\"]:\n",
    "            comp[\"minimum_value\"], comp[\"maximum_value\"] = (\n",
    "                comp[\"maximum_value\"], comp[\"minimum_value\"])\n",
    "\n",
    "        # Unit: set only when any ° or C is present\n",
    "        if _UNIT_PRESENT_RGX.search(raw):\n",
    "            comp[\"unit\"] = \"Cel\"\n",
    "    else:\n",
    "        comp[\"unparsed_text\"] = raw\n",
    "\n",
    "    return comp\n",
    "\n"
   ],
   "id": "cb074511039bdff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "#  pre-compiled regex\n",
    "#     ┌──────── dir            (above / below, case-insensitive)\n",
    "#     │┌─────── optional leading ° / C (any whitespace)\n",
    "#     ││   ┌─── number (int or float, signed)\n",
    "#     ││   │┌─ optional trailing ° / C\n",
    "HALF_RANGE_RX = re.compile(\n",
    "    r\"\"\"\n",
    "    (?P<dir>above|below)               # direction keyword\n",
    "    \\s*                                # optional blanks\n",
    "    (?:[°º]\\s* C? \\s*)?                # optional “°C” (before number)\n",
    "    (?P<val>[-+]?\\d+(?:\\.\\d+)?)        # the numeric value\n",
    "    \\s* (?:[°º]\\s* C?)?                # optional “°C” (after number)\n",
    "    \"\"\",\n",
    "    re.I | re.VERBOSE,\n",
    ")\n",
    "\n",
    "\n",
    "def parse_half_range_celsius_degree_value(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse strings like\n",
    "        “above 45°C”, “below 30 C”, “ABOVE °​60C”, …\n",
    "    Return a dict suitable for add_parsegroup().\n",
    "    \"\"\"\n",
    "    if text is None:  # guard against None\n",
    "        raise ValueError(\"input is None\")\n",
    "\n",
    "    m = HALF_RANGE_RX.search(text.strip())\n",
    "    if m is None:\n",
    "        raise ValueError(f\"cannot parse half-range: {text!r}\")\n",
    "\n",
    "    direction = m.group(\"dir\").lower()\n",
    "    value = float(m.group(\"val\"))\n",
    "\n",
    "    out = {\n",
    "        \"component_text\": text.strip(),  # verbatim (for provenance)\n",
    "        \"unit\": \"Cel\",\n",
    "    }\n",
    "    if direction == \"above\":\n",
    "        out[\"minimum_value\"] = value  # only the lower bound\n",
    "    else:  # below\n",
    "        out[\"maximum_value\"] = value  # only the upper bound\n",
    "\n",
    "    return out"
   ],
   "id": "61d9250c56ec2d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================================\n",
    "#  Updated “generic RDF writer”\n",
    "#\n",
    "#  •  to_parse   – the string that was actually fed into the temperature\n",
    "#                  parser (usually the cleaned / normalised token)\n",
    "#  •  to_claim   – the *verbatim* text that appears in the source; it is\n",
    "#                  what you want in ENV.raw_text for provenance\n",
    "#\n",
    "#  If you pass only to_parse, it is used for both purposes (same behaviour\n",
    "#  as the previous version).  If you supply two different strings you keep\n",
    "#  the original-substring provenance while still parsing the cleaned text.\n",
    "# ==========================================================================\n",
    "\n",
    "\n",
    "ENV = Namespace(\"http://example.org/env-parse#\")\n",
    "BASE_PG = Namespace(\"http://example.org/pg/\")\n",
    "\n",
    "\n",
    "def add_parsegroup(comp: Dict,\n",
    "                   *,\n",
    "                   graph: Graph,\n",
    "                   to_parse: str,\n",
    "                   to_claim: Optional[str] = None,\n",
    "                   pg_uri: Optional[URIRef] = None) -> URIRef:\n",
    "    \"\"\"\n",
    "    Insert one ParseGroup + ONE ParseComponent into `graph`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    comp      : dict\n",
    "        Output of your (spot / range) parser.  Must contain at least\n",
    "        \"component_text\"; may contain the optional numeric & label keys.\n",
    "    graph     : rdflib.Graph\n",
    "        The graph that will receive the triples.\n",
    "    to_parse  : str\n",
    "        The cleaned string that was actually parsed.\n",
    "    to_claim  : str | None\n",
    "        The *verbatim* substring that will be stored as ENV.raw_text.\n",
    "        If None, `to_parse` is used (back-compat with older code).\n",
    "    pg_uri    : URIRef | None\n",
    "        Use an existing URI for the ParseGroup or let the function\n",
    "        mint a fresh one under BASE_PG.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    URIRef\n",
    "        The (possibly freshly minted) URI of the new ParseGroup.\n",
    "    \"\"\"\n",
    "    to_claim = to_claim if to_claim is not None else to_parse\n",
    "\n",
    "    pg_uri = pg_uri or BASE_PG[str(uuid4())]\n",
    "    comp_bn = BNode()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ParseGroup triples\n",
    "    # ------------------------------------------------------------------\n",
    "    graph.add((pg_uri, RDF.type, ENV.ParseGroup))\n",
    "    graph.add((pg_uri, ENV.raw_text, Literal(to_claim, datatype=XSD.string)))\n",
    "    # (optional) store the text-that-was-parsed as well — can be handy\n",
    "    if to_parse != to_claim:\n",
    "        graph.add((pg_uri, ENV.parse_text, Literal(to_parse, datatype=XSD.string)))\n",
    "\n",
    "    graph.add((pg_uri, ENV.parse_component, comp_bn))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ParseComponent – common triples\n",
    "    # ------------------------------------------------------------------\n",
    "    graph.add((comp_bn, RDF.type, ENV.ParseComponent))\n",
    "    graph.add((comp_bn, ENV.component_text,\n",
    "               Literal(comp[\"component_text\"], datatype=XSD.string)))\n",
    "\n",
    "    # optional numeric / label / leftover fields -----------------------\n",
    "    field_map = {\n",
    "        \"minimum_value\": ENV.minimum_value,\n",
    "        \"maximum_value\": ENV.maximum_value,\n",
    "        \"spot_value\": ENV.spot_value,\n",
    "        \"unit\": ENV.unit,\n",
    "        \"categorical_label\": ENV.categorical_label,\n",
    "        \"qualifier_label\": ENV.qualifier_label,\n",
    "        \"unparsed_text\": ENV.unparsed_text,\n",
    "    }\n",
    "    for k, prop in field_map.items():\n",
    "        if k in comp:\n",
    "            lit = (Literal(str(comp[k]), datatype=XSD.decimal)\n",
    "                   if k.endswith(\"_value\") else\n",
    "                   Literal(comp[k], datatype=XSD.string))\n",
    "            graph.add((comp_bn, prop, lit))\n",
    "\n",
    "    return pg_uri"
   ],
   "id": "cdf2e5bef474ece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "T = TypeVar(\"T\")  # parsed result of any type\n",
    "ParserFunc = Callable[[str], T]"
   ],
   "id": "b088d3b9e20ffb41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------------------------\n",
    "#  helper: take a row, run the domain parser, write the triples\n",
    "# ----------------------------------------------------------------------\n",
    "def _row_to_graph(\n",
    "        row,\n",
    "        *,\n",
    "        graph: Graph,\n",
    "        s_col: str,\n",
    "        p_col: str,\n",
    "        to_parse_col: str,\n",
    "        to_claim_col: str | None,\n",
    "        parser: Callable[[str], dict],\n",
    "):\n",
    "    subj_uri = URIRef(row[s_col])\n",
    "    pred_uri = URIRef(row[p_col])\n",
    "    to_parse = row[to_parse_col]\n",
    "    to_claim = row[to_claim_col] if to_claim_col is not None else None\n",
    "\n",
    "    # run the domain-specific parser (returns dict with component_text etc.)\n",
    "    comp = parser(to_parse)\n",
    "\n",
    "    # add_parsegroup inserts all ParseGroup / ParseComponent triples\n",
    "    add_parsegroup(\n",
    "        comp,\n",
    "        graph=graph,\n",
    "        to_parse=to_parse,\n",
    "        to_claim=to_claim,  # <- may be None → falls back to to_parse\n",
    "        pg_uri=None,  # new URI minted automatically\n",
    "    )\n",
    "\n",
    "    # link the ParseGroup to the subject / predicate (your vocab!)\n",
    "    # (remove / change if you already do this elsewhere)\n",
    "    pg_uri = BASE_PG[str(uuid4())]\n",
    "    graph.add((subj_uri, pred_uri, pg_uri))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  main helper\n",
    "# ----------------------------------------------------------------------\n",
    "def _pick_claim_column(\n",
    "        df: pd.DataFrame,\n",
    "        *,\n",
    "        to_parse_col: str,\n",
    "        to_claim_col: str | None,\n",
    "        auto_suffixes: tuple[str, ...] = (\"_intact\", \"_raw\", \"_orig\"),\n",
    ") -> str | None:\n",
    "    \"\"\"\n",
    "    Decide which column to take for the verbatim `raw_text`.\n",
    "\n",
    "    • If `to_claim_col` was supplied and exists → use it.\n",
    "    • Else look for <to_parse_col><suffix> in *df* (common patterns).\n",
    "    • Else return None  → add_parsegroup() will fall back to parse string.\n",
    "    \"\"\"\n",
    "    if to_claim_col and to_claim_col in df.columns:\n",
    "        return to_claim_col\n",
    "\n",
    "    for suf in auto_suffixes:\n",
    "        cand = f\"{to_parse_col}{suf}\"\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "\n",
    "    return None  # let add_parsegroup() fall back\n",
    "\n",
    "\n",
    "def parse_triple_dataframe(\n",
    "        df: pd.DataFrame,\n",
    "        *,\n",
    "        s_col: str = \"subject\",\n",
    "        p_col: str = \"predicate\",\n",
    "        to_parse_col: str = \"object\",\n",
    "        to_claim_col: str | None = None,  #  now truly optional\n",
    "        parser: Callable[[str], dict],\n",
    "        graph: Graph | None = None,\n",
    ") -> Graph:\n",
    "    \"\"\"\n",
    "    Iterate over *df*, parse each row’s value and emit ParseGroup triples.\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        graph = Graph()\n",
    "\n",
    "    # decide once which column to take for raw_text\n",
    "    claim_col = _pick_claim_column(df,\n",
    "                                   to_parse_col=to_parse_col,\n",
    "                                   to_claim_col=to_claim_col)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        subj_uri = URIRef(row[s_col])\n",
    "        pred_uri = URIRef(row[p_col])\n",
    "\n",
    "        to_parse = row[to_parse_col]\n",
    "        to_claim = row[claim_col] if claim_col else None\n",
    "\n",
    "        comp = parser(to_parse)\n",
    "\n",
    "        pg_uri = add_parsegroup(\n",
    "            comp,\n",
    "            graph=graph,\n",
    "            to_parse=to_parse,\n",
    "            to_claim=to_claim,  # may be None → fallback inside helper\n",
    "        )\n",
    "\n",
    "        graph.add((subj_uri, pred_uri, pg_uri))\n",
    "\n",
    "    return graph"
   ],
   "id": "45c7f5f51926b327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Temperature-list splitter + unit normaliser\n",
    "───────────────────────────────────────────\n",
    "For strings such as\n",
    "\n",
    "    \"4 °C, 45°C\"          →  [\"4 C\", \"45 C\"]\n",
    "    \"4, 52 °C\"            →  [\"4 C\", \"52 C\"]\n",
    "    \"25°C, 30°C\"          →  [\"25 C\", \"30 C\"]\n",
    "\n",
    "1. Decide which temperature unit is present (C / F / K).\n",
    "2. Split on the internal comma / “and” / “or”.\n",
    "3. Remove every occurrence of the unit or degree glyph inside each part.\n",
    "4. Re-append the normalised unit to every part.\n",
    "5. `explode_value_column()` does the usual DataFrame explode, now\n",
    "   delegating the heavy lifting to the new splitter class.\n",
    "\"\"\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Local helpers\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "_SPLIT_RX: Final[Pattern] = re.compile(r\"\\s*(?:,|\\band\\b|\\bor\\b)\\s*\", re.I)\n",
    "_TERM_PUNCT_RX: Final[Pattern] = re.compile(r\"[.;,\\s]+$\")\n",
    "\n",
    "_UNIT_STRIP_RXES: Final[tuple[Pattern, ...]] = (\n",
    "    CELSIUS_RX,\n",
    "    FAHREN_RX,\n",
    "    KELVIN_RX,\n",
    "    WORD_DEGREE_RX,\n",
    "    DEGREE_RX,\n",
    ")\n",
    "\n",
    "\n",
    "class TemperatureSplitter:\n",
    "    \"\"\"\n",
    "    Splitter + unit-normaliser for comma-separated temperature lists.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> TemperatureSplitter().split(\"4 °C, 45°C\")\n",
    "    ['4 C', '45 C']\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = ()\n",
    "\n",
    "    # ––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "    #  Public interface\n",
    "    # ––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "    def split(self, text: str | None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split `text` into normalised temperature parts.\n",
    "\n",
    "        • If `text` is None, returns [].\n",
    "        • If `text` is empty/whitespace, returns [''].\n",
    "        • If the heuristic decides that no split is required,\n",
    "          the cleaned value (unit normalised once) is wrapped in a list.\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return []\n",
    "\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            return [\"\"]\n",
    "\n",
    "        if not self._should_split(text):\n",
    "            return [self._clean(text, self._detect_unit(text))]\n",
    "\n",
    "        unit = self._detect_unit(text)\n",
    "        parts: Iterable[str] = (p for p in _SPLIT_RX.split(text) if p)\n",
    "\n",
    "        return [self._clean(p, unit) for p in parts] or [text]\n",
    "\n",
    "    # ––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "    #  Internals\n",
    "    # ––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "    @staticmethod\n",
    "    def _detect_unit(text: str) -> str:\n",
    "        \"\"\"Return 'C', 'F', 'K' or ''.\"\"\"\n",
    "        if FAHREN_RX.search(text):\n",
    "            return \"F\"\n",
    "        if KELVIN_RX.search(text):\n",
    "            return \"K\"\n",
    "        if CELSIUS_RX.search(text) or WORD_DEGREE_RX.search(text):\n",
    "            return \"C\"\n",
    "        return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _should_split(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Heuristic: does *text* look like a list of temperature values?\n",
    "        (number(s) + unit + at least one recognised separator)\n",
    "        \"\"\"\n",
    "        # ignore a possible trailing comma when looking for a separator\n",
    "        searchable = text[:-1] if text.endswith(\",\") else text\n",
    "\n",
    "        has_separator = bool(_SPLIT_RX.search(searchable))\n",
    "        has_number = bool(DIGIT_RX.search(text))\n",
    "        has_unit = any(\n",
    "            rx.search(text) for rx in (CELSIUS_RX, FAHREN_RX, KELVIN_RX)\n",
    "        )\n",
    "        return has_separator and has_number and has_unit\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean(part: str, unit: str) -> str:\n",
    "        \"\"\"Strip units & punctuation from *part*, then append *unit*.\"\"\"\n",
    "        for rx in _UNIT_STRIP_RXES:\n",
    "            part = rx.sub(\"\", part)\n",
    "\n",
    "        part = _TERM_PUNCT_RX.sub(\"\", part).strip()\n",
    "        return f\"{part} {unit}\".strip() if unit else part\n",
    "\n",
    "\n",
    "# Singleton used by the thin convenience wrapper below\n",
    "_DEFAULT_SPLITTER: Final[TemperatureSplitter] = TemperatureSplitter()\n",
    "\n",
    "\n",
    "def split_temperature_values(text: str | None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Backwards-compatibility wrapper around the `TemperatureSplitter`.\n",
    "    \"\"\"\n",
    "    return _DEFAULT_SPLITTER.split(text)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  DataFrame helper\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def explode_value_column(\n",
    "        df: pd.DataFrame,\n",
    "        value_col: str = \"object\",\n",
    "        *,\n",
    "        drop_original: bool = True,\n",
    "        splitter: TemperatureSplitter | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split / normalise *value_col*, explode, keep both the split parts and\n",
    "    the untouched original value.\n",
    "\n",
    "    Resulting columns\n",
    "    -----------------\n",
    "    {value_col}_split   – individual, normalised pieces\n",
    "    {value_col}_intact  – original string (repeated on every exploded row)\n",
    "    \"\"\"\n",
    "    if value_col not in df.columns:\n",
    "        raise KeyError(f\"{value_col!r} is not a column of the DataFrame\")\n",
    "\n",
    "    splitter = splitter or _DEFAULT_SPLITTER\n",
    "    split_col, intact_col = f\"{value_col}_split\", f\"{value_col}_intact\"\n",
    "\n",
    "    out = (\n",
    "        df.assign(\n",
    "            **{\n",
    "                intact_col: df[value_col],  # original\n",
    "                split_col: df[value_col].apply(splitter.split),\n",
    "            }\n",
    "        )\n",
    "        .explode(split_col, ignore_index=True)\n",
    "    )\n",
    "\n",
    "    return out.drop(columns=value_col) if drop_original else out"
   ],
   "id": "f90230b23fe34069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1.  build a frequency-rank lookup in two lines\n",
    "# ------------------------------------------------------------\n",
    "FLAG_COLS = [c for c in enriched.columns if c.startswith(\"has_\")]\n",
    "\n",
    "# each row → frozenset of flags whose value is True\n",
    "enriched[\"_flag_set\"] = enriched[FLAG_COLS].apply(\n",
    "    lambda r: frozenset(r.index[r]), axis=1\n",
    ")\n",
    "\n",
    "# frequency table:  flag-set  →  count, then convert to rank (1 = most common)\n",
    "freq = (\n",
    "    enriched[\"_flag_set\"]\n",
    "    .value_counts()  # Series: index = flag-set, value = count\n",
    "    .rank(ascending=False, method=\"dense\")  # 1, 2, 3, …  (dense ranking)\n",
    "    .astype(int)  # make it nice integers\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  quick helper\n",
    "# ------------------------------------------------------------\n",
    "def freq_rank_of_flags(flags) -> int | None:\n",
    "    \"\"\"Return frequency-rank (1 = most common) for a list of flags.\"\"\"\n",
    "    return freq.get(frozenset(flags))\n"
   ],
   "id": "db1b32f4e0f62516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "ef66ba78863e251c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1.  cookbook that drives the whole parsing pipeline\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "TASKS = [\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            ['has_any_hyphen', 'has_celsius', 'has_degree_symbol', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_any_hyphen', 'has_celsius', 'has_degree_symbol', 'has_number'],\n",
    "            ['has_any_hyphen', 'has_celsius', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_any_hyphen', 'has_celsius', 'has_number'],\n",
    "            ['has_any_hyphen', 'has_degree_symbol', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_any_hyphen', 'has_degree_symbol', 'has_number'],\n",
    "        ],\n",
    "        parser=parse_hyphen_range_celsius_degree_values,\n",
    "        to_parse=\"object\",\n",
    "        to_claim=None,  # let add_parsegroup() fall back\n",
    "        explode=False,\n",
    "        out_file=\"parse_hyphen_range_celsius_degree_values.ttl\",\n",
    "        subsetting=\"exact\",\n",
    "    ),\n",
    "\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_number'],\n",
    "            ['has_celsius', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_number'],\n",
    "            ['has_degree_symbol', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_degree_symbol', 'has_number'],\n",
    "        ],\n",
    "        parser=parse_spot_celsius_degree_value,\n",
    "        to_parse=\"object\",\n",
    "        to_claim=None,\n",
    "        explode=False,\n",
    "        out_file=\"parse_spot_celsius_degree_value.ttl\",\n",
    "        subsetting=\"exact\",\n",
    "    ),\n",
    "\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            ['has_ophil'],\n",
    "            ['has_toleran'],\n",
    "        ],\n",
    "        parser=parse_categorical_label,\n",
    "        to_parse=\"object\",\n",
    "        to_claim=None,\n",
    "        explode=False,\n",
    "        out_file=\"parse_categorical_label.ttl\",\n",
    "        subsetting=\"at_least\",\n",
    "    ),\n",
    "\n",
    "    # rank 4  -----------------------------------------------------------\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            ['has_to', 'has_celsius', 'has_degree_symbol', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_to', 'has_celsius', 'has_degree_symbol', 'has_number'],\n",
    "            ['has_to', 'has_celsius', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_to', 'has_celsius', 'has_number'],\n",
    "            ['has_to', 'has_degree_symbol', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_to', 'has_degree_symbol', 'has_number'],\n",
    "        ],\n",
    "        parser=parse_to_temperature_range,\n",
    "        to_parse=\"object\",\n",
    "        to_claim=None,\n",
    "        explode=False,\n",
    "        out_file=\"parse_to_temperature_range.ttl\",\n",
    "        subsetting=\"exact\",\n",
    "    ),\n",
    "\n",
    "    # rank 5 / 6 / 7  ---------------------------------------------------\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            ['has_celsius', 'has_and', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_and', 'has_number'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_and', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_and', 'has_number'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_internal_comma', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_internal_comma', 'has_number'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_or', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_or', 'has_number'],\n",
    "            ['has_celsius', 'has_internal_comma', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_internal_comma', 'has_number'],\n",
    "            ['has_celsius', 'has_or', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_or', 'has_number'],\n",
    "            ['has_degree_symbol', 'has_and', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_degree_symbol', 'has_and', 'has_number'],\n",
    "            ['has_degree_symbol', 'has_internal_comma', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_degree_symbol', 'has_internal_comma', 'has_number'],\n",
    "            ['has_degree_symbol', 'has_or', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_degree_symbol', 'has_or', 'has_number'],\n",
    "        ],\n",
    "        parser=parse_spot_celsius_degree_value,\n",
    "        to_parse=\"object_split\",\n",
    "        to_claim=\"object_intact\",\n",
    "        explode=True,\n",
    "        out_file=\"parse_exploded_temperature_range.ttl\",\n",
    "        subsetting=\"exact\",\n",
    "    ),\n",
    "\n",
    "    # rank 9 & 11  ------------------------------------------------------\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_init_above', 'has_above', 'has_number',\n",
    "             'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_init_above', 'has_above', 'has_number'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_init_below', 'has_below', 'has_number',\n",
    "             'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_degree_symbol', 'has_init_below', 'has_below', 'has_number'],\n",
    "            ['has_celsius', 'has_init_above', 'has_above', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_init_above', 'has_above', 'has_number'],\n",
    "            ['has_celsius', 'has_init_below', 'has_below', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_celsius', 'has_init_below', 'has_below', 'has_number'],\n",
    "            ['has_degree_symbol', 'has_init_above', 'has_above', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_degree_symbol', 'has_init_above', 'has_above', 'has_number'],\n",
    "            ['has_degree_symbol', 'has_init_below', 'has_below', 'has_number', 'has_terminal_punctuation'],\n",
    "            ['has_degree_symbol', 'has_init_below', 'has_below', 'has_number'],\n",
    "        ],\n",
    "        parser=parse_half_range_celsius_degree_value,\n",
    "        to_parse=\"object\",\n",
    "        to_claim=None,\n",
    "        explode=False,\n",
    "        out_file=\"parse_half_range_celsius_degree_value.ttl\",\n",
    "        subsetting=\"exact\",\n",
    "    ),\n",
    "\n",
    "    # rank ???  ------------------------------------------------------\n",
    "    dict(\n",
    "        flag_sets=[\n",
    "            [\n",
    "                'has_above',\n",
    "                'has_below',\n",
    "                'has_celsius',\n",
    "                'has_degree_symbol',\n",
    "                'has_internal_comma',\n",
    "                'has_number',\n",
    "            ],\n",
    "            [\n",
    "                'has_above',\n",
    "                'has_below',\n",
    "                'has_celsius',\n",
    "                'has_degree_symbol',\n",
    "                'has_and',\n",
    "                'has_number',\n",
    "            ],\n",
    "            [\n",
    "                'has_above',\n",
    "                'has_below',\n",
    "                'has_celsius',\n",
    "                'has_degree_symbol',\n",
    "                'has_or',\n",
    "                'has_number',\n",
    "            ]\n",
    "        ],\n",
    "        parser=parse_half_range_celsius_degree_value,\n",
    "        to_parse=\"object_split\",\n",
    "        to_claim=\"object_intact\",\n",
    "        explode=True,\n",
    "        out_file=\"parse_half_range_celsius_degree_value.ttl\",\n",
    "        subsetting=\"at_least\",\n",
    "        false_flags=['has_at', 'has_any_other_text'],\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n"
   ],
   "id": "c5262423e3d15c35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2.  main loop: parse, serialize per task, collect graphs\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "graphs = []  # one graph per task → later union\n",
    "cumulative = 0\n",
    "processed_idx = set()\n",
    "\n"
   ],
   "id": "1267e5fd568b13a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for task in TASKS:\n",
    "    # 2.1  show ranks once per flag-set\n",
    "    for fs in task[\"flag_sets\"]:\n",
    "        print(f\"{fs}  is ranked  {freq_rank_of_flags(fs)}\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2.2  matching rows (union of all flag-sets in the task)\n",
    "    # -----------------------------------------------------------------\n",
    "    false_flags = task.get('false_flags')          # will be None if key absent\n",
    "\n",
    "    if task['subsetting'] == \"exact\":\n",
    "        subset = (\n",
    "            pd.concat(\n",
    "                select_exact(enriched, fs, false_flags=false_flags)    # <── pass it\n",
    "                for fs in task[\"flag_sets\"]\n",
    "            )\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "\n",
    "    elif task['subsetting'] == \"at_least\":\n",
    "        subset = (\n",
    "            pd.concat(\n",
    "                select_at_least(enriched, fs, false_flags=false_flags) # <── pass it\n",
    "                for fs in task[\"flag_sets\"]\n",
    "            )\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Illegal subsetting value: \"\n",
    "                         f\"{task['subsetting']!r}\")\n",
    "\n",
    "    processed_idx.update(subset.index)  # 2️⃣ remember them\n",
    "    pre_explode_len = len(subset)  # ← this is what we want to count\n",
    "\n",
    "    # optional explode for parsing only\n",
    "    if task[\"explode\"]:\n",
    "        subset = explode_value_column(subset, value_col=\"object\")\n",
    "\n",
    "    # 2.3  run the domain parser → per-task graph\n",
    "    g = parse_triple_dataframe(\n",
    "        subset,\n",
    "        s_col=\"subject\",\n",
    "        p_col=\"predicate\",\n",
    "        to_parse_col=task[\"to_parse\"],\n",
    "        to_claim_col=task[\"to_claim\"],\n",
    "        parser=task[\"parser\"],\n",
    "    )\n",
    "    graphs.append(g)\n",
    "\n",
    "    # # optional individual serialisation\n",
    "    # g.serialize(task[\"out_file\"], format=\"turtle\")\n",
    "\n",
    "    # accumulate coverage using the pre-explode size\n",
    "    cumulative += pre_explode_len\n",
    "    print(\"rows added (unique):\", pre_explode_len,\n",
    "          \"cumulative:\", cumulative, \"\\n\")\n",
    "\n"
   ],
   "id": "3e8db3ba771d88e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3.  after-the-fact union  →  MASTER graph\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "master = Graph()\n"
   ],
   "id": "4d06e4236f6afbc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for g in graphs:\n",
    "    master += g  # rdflib unions duplicates automatically\n"
   ],
   "id": "d0f61178686a4507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "master.serialize(ttl_out, format=\"turtle\")\n"
   ],
   "id": "f2cd84fe3dc7b672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"overall coverage:\", cumulative / len(enriched))",
   "id": "5e4aaf9b4ceff9bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "unprocessed = enriched.loc[~enriched.index.isin(processed_idx)]",
   "id": "b15c25c56075a98d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "unprocessed.shape",
   "id": "3fee5b63b52bb933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "unprocessed.to_csv(unparsed_out, index=False)",
   "id": "8bdcd03991c24ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sorted(enriched.columns)",
   "id": "23ee21b14db51a0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5570993d244ec03d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
