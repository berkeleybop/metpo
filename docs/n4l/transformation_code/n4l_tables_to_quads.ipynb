{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from rdflib import Dataset, Literal, URIRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46335522b31a73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle both interactive and CLI (e.g., papermill) execution\n",
    "notebook_path = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# Project root is the parent of `metpo/`, even though the notebook lives in `metpo/`\n",
    "project_root = notebook_path if (notebook_path / \"assets\").is_dir() else notebook_path.parent\n",
    "assets_dir = project_root / \"assets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971174b17f8caa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "n4l_data_directory = assets_dir / \"N4L_phenotypic_ontology_2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4571d839c55f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_config_path = assets_dir / \"n4l-xlsx-parsing-config.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a6ce01ab0ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_mapping_normalization_file = assets_dir / \"n4l_predicate_mapping_normalization.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27fad2dadbd61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_out = project_root / \"local\" / \"n4l-tables.nq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a4cbece26f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n4l_prefix = \"http://example.com/n4l/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8854b6e8ad1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_csv(predicate_mapping_normalization_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2a9001fff07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimited_text_configs = [\n",
    "    {\n",
    "        \"filename\": \"N4L_Taxonomy_20220802_pruned.tsv\",\n",
    "        \"path\": f\"{n4l_data_directory}/N4L_Taxonomy_20220802_pruned.tsv\",\n",
    "        \"id_column\": \"N4LID\",\n",
    "        \"delimiter\": \"\\t\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"N4L_Taxonomy_20220802.tsv\",\n",
    "        \"path\": f\"{n4l_data_directory}/N4L_Taxonomy_20220802.tsv\",\n",
    "        \"id_column\": \"N4LID\",\n",
    "        \"delimiter\": \"\\t\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"reference_id_mapping.csv\",\n",
    "        \"path\": f\"{n4l_data_directory}/reference_id_mapping.csv\",\n",
    "        \"id_column\": \"refid\",\n",
    "        \"delimiter\": \",\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd9cfd0d514063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_iri_component(value: str) -> str:\n",
    "    # Replace all whitespace characters and colons with underscores\n",
    "    cleaned = re.sub(r\"[\\s:]+\", \"_\", value.strip())\n",
    "    # Percent-encode everything else, but preserve underscores\n",
    "    return quote(cleaned, safe=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9d78bfb627f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(val):\n",
    "    \"\"\"Convert common string values to boolean.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return False\n",
    "    return str(val).strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40edf4e1b52584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transposed_sheet(df, id_column):\n",
    "    df = df.transpose()\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].reset_index(drop=True)\n",
    "\n",
    "    if id_column not in df.columns:\n",
    "        raise ValueError(f\"'{id_column}' not found in transposed headers\")\n",
    "\n",
    "    df = df.dropna(subset=[id_column])\n",
    "    melted = df.melt(id_vars=[id_column], var_name=\"predicate\", value_name=\"object_value\")\n",
    "    return melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaedd9e7a9ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_standard_sheet(df, id_column):\n",
    "    if id_column not in df.columns:\n",
    "        raise ValueError(f\"'{id_column}' not found in standard headers\")\n",
    "\n",
    "    df = df.dropna(subset=[id_column])\n",
    "    melted = df.melt(id_vars=[id_column], var_name=\"predicate\", value_name=\"object_value\")\n",
    "    return melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4467bea3f0495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_object_term(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    val = str(val).strip()\n",
    "    parsed = urlparse(val)\n",
    "    if parsed.scheme in (\"http\", \"https\") and parsed.netloc and \" \" not in val:\n",
    "        try:\n",
    "            return URIRef(val)  # Only if it's really URI-safe\n",
    "        except:\n",
    "            pass\n",
    "    return Literal(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb598e967da0c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_frames = []\n",
    "melted_dropped_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210da3d5046b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in delimited_text_configs:\n",
    "    try:\n",
    "        df = pd.read_csv(config[\"path\"], sep=config[\"delimiter\"], low_memory=False)\n",
    "\n",
    "        graph_iri = f\"{n4l_prefix}{safe_iri_component(config['filename'].strip())}\"\n",
    "\n",
    "        # Remove exact duplicates before anything else\n",
    "        before = df.shape[0]\n",
    "        df = df.drop_duplicates()\n",
    "        after = df.shape[0]\n",
    "        if after < before:\n",
    "            print(f\"[QC] {config['filename']} - Removed {before - after} fully duplicated rows\")\n",
    "\n",
    "        id_column = config[\"id_column\"]\n",
    "        if id_column not in df.columns:\n",
    "            print(\n",
    "                f\"[ERROR] {config['filename']} - ID column '{id_column}' not found. Available columns: {df.columns.tolist()}\")\n",
    "            continue\n",
    "\n",
    "        # Drop and log missing ID rows\n",
    "        missing_id_rows = df[df[id_column].isna()]\n",
    "        if not missing_id_rows.empty:\n",
    "            print(f\"[QC] {config['filename']} - Dropped {len(missing_id_rows)} rows missing '{id_column}'\")\n",
    "            melted_missing = missing_id_rows.melt(var_name=\"predicate\", value_name=\"object_value\")\n",
    "            melted_missing[\"subject\"] = None\n",
    "            melted_missing[\"source_file\"] = config[\"filename\"]\n",
    "            melted_missing[\"drop_reason\"] = \"missing_id\"\n",
    "            melted_missing[\"graph\"] = graph_iri\n",
    "            melted_dropped_frames.append(melted_missing)\n",
    "\n",
    "        df = df.dropna(subset=[id_column])\n",
    "\n",
    "        # Drop and log duplicated IDs\n",
    "        duplicated_mask = df[id_column].duplicated(keep=False)\n",
    "        if duplicated_mask.any():\n",
    "            duplicated_ids = df.loc[duplicated_mask, id_column].unique()\n",
    "            print(\n",
    "                f\"[DUPLICATES] {config['filename']} - {duplicated_mask.sum()} duplicate rows on '{id_column}' → {duplicated_ids.tolist()}\")\n",
    "            melted_dupes = df.loc[duplicated_mask].melt(var_name=\"predicate\", value_name=\"object_value\")\n",
    "            melted_dupes[\"subject\"] = df.loc[duplicated_mask, id_column].values.repeat(len(df.columns) - 1)\n",
    "            melted_dupes[\"source_file\"] = config[\"filename\"]\n",
    "            melted_dupes[\"drop_reason\"] = \"duplicate_id\"\n",
    "            melted_dupes[\"graph\"] = graph_iri\n",
    "            melted_dropped_frames.append(melted_dupes)\n",
    "            df = df[~duplicated_mask]\n",
    "        else:\n",
    "            print(f\"[DUPLICATES] {config['filename']} - No duplicates in '{id_column}'\")\n",
    "\n",
    "        # Melt and append\n",
    "        melted = df.melt(id_vars=[id_column], var_name=\"predicate\", value_name=\"object_value\")\n",
    "        melted = melted.rename(columns={id_column: \"subject\"})\n",
    "        melted = melted.dropna(subset=[\"subject\", \"predicate\", \"object_value\"])\n",
    "        melted[\"subject\"] = melted[\"subject\"].astype(str).apply(\n",
    "            lambda x: f\"{n4l_prefix}{safe_iri_component(x.strip())}\")\n",
    "        melted[\"predicate\"] = melted[\"predicate\"].astype(str).apply(\n",
    "            lambda x: f\"{n4l_prefix}{safe_iri_component(x.strip())}\")\n",
    "        melted[\"source_file\"] = config[\"filename\"]\n",
    "        melted[\"graph\"] = graph_iri\n",
    "        melted_frames.append(melted)\n",
    "        print(f\"[INFO] {config['filename']} → {melted.shape[0]} melted rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed processing {config['filename']} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a6f6efc35977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_sheet_configs = pd.read_csv(xlsx_config_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05691a8eca9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_sheet_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ff837b87cbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in xlsx_sheet_configs.iterrows():\n",
    "    print(row)\n",
    "\n",
    "    skip = str_to_bool(row.get(\"skip\", False))\n",
    "    spo_already = str_to_bool(row.get(\"spo_already\", False))\n",
    "    requires_transposition = str_to_bool(row.get(\"requires_transposition\", False))\n",
    "    id_column = row.get(\"id_column\")\n",
    "    composite_columns = None\n",
    "\n",
    "    if skip:\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(n4l_data_directory, row[\"filename\"])\n",
    "    sheet_name = row[\"sheet_name\"]\n",
    "    graph_iri = f\"{n4l_prefix}{safe_iri_component(row['filename'].strip())}/{safe_iri_component(sheet_name.strip())}\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None if requires_transposition else 0)\n",
    "\n",
    "        if requires_transposition:\n",
    "            df = df.transpose()\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "\n",
    "        df.columns = df.columns.map(lambda x: str(x).strip())  # Normalize column names\n",
    "\n",
    "        if spo_already:\n",
    "            if df.shape[1] != 3:\n",
    "                print(\n",
    "                    f\"[ERROR] {row['filename']}:{sheet_name} - Expected 3 columns for SPO format, found {df.shape[1]}\")\n",
    "                continue\n",
    "            df.columns = [\"subject\", \"predicate\", \"object_value\"]\n",
    "            df = df.dropna(subset=[\"subject\", \"predicate\", \"object_value\"])\n",
    "            df[\"subject\"] = df[\"subject\"].astype(str).apply(lambda x: f\"{n4l_prefix}{safe_iri_component(x.strip())}\")\n",
    "            df[\"predicate\"] = df[\"predicate\"].astype(str).apply(\n",
    "                lambda x: f\"{n4l_prefix}{safe_iri_component(x.strip())}\")\n",
    "            df[\"source_file\"] = row[\"filename\"]\n",
    "            df[\"source_sheet\"] = sheet_name\n",
    "            df[\"graph\"] = graph_iri\n",
    "            melted_frames.append(df)\n",
    "            print(f\"[INFO] {row['filename']}:{sheet_name} (SPO) → {df.shape[0]} rows\")\n",
    "            continue\n",
    "\n",
    "        # --- Composite ID handling (improved) ---\n",
    "        if isinstance(id_column, str) and \"|\" in id_column:\n",
    "            composite_columns = [col.strip() for col in id_column.split(\"|\")]\n",
    "\n",
    "            normalized_cols = {str(col).strip(): col for col in df.columns}\n",
    "            missing = [col for col in composite_columns if col not in normalized_cols]\n",
    "            if missing:\n",
    "                print(f\"[ERROR] {row['filename']}:{sheet_name} - Missing composite ID columns: {missing}\")\n",
    "                continue\n",
    "\n",
    "            matched = [normalized_cols[col] for col in composite_columns]\n",
    "            id_column = \"_\".join(composite_columns)\n",
    "\n",
    "            df[matched] = df[matched].astype(str).applymap(lambda v: v.strip() if isinstance(v, str) else v)\n",
    "\n",
    "            # Log partial composite IDs\n",
    "            partial_ids = df[matched].isna().any(axis=1) & ~df[matched].isna().all(axis=1)\n",
    "            if partial_ids.any():\n",
    "                print(f\"[QC] {row['filename']}:{sheet_name} - {partial_ids.sum()} rows with partial composite IDs\")\n",
    "\n",
    "            # Join only valid components\n",
    "            def safe_join(vals):\n",
    "                return \"_\".join([str(v).strip() for v in vals if v and str(v).strip().lower() != \"nan\"])\n",
    "\n",
    "            df[id_column] = df[matched].agg(safe_join, axis=1)\n",
    "\n",
    "            # Drop rows where all components were missing\n",
    "            blank_ids = df[matched].isna().all(axis=1)\n",
    "            if blank_ids.any():\n",
    "                print(f\"[QC] {row['filename']}:{sheet_name} - Dropped {blank_ids.sum()} rows with blank synthetic ID\")\n",
    "                df = df[~blank_ids]\n",
    "\n",
    "            print(f\"[INFO] Created synthetic ID column '{id_column}' from: {composite_columns}\")\n",
    "\n",
    "        else:\n",
    "            if pd.isna(id_column):\n",
    "                print(f\"[ERROR] {row['filename']}:{sheet_name} - ID column is NaN\")\n",
    "                continue\n",
    "\n",
    "            normalized_cols = {str(col).strip(): col for col in df.columns}\n",
    "            if id_column.strip() not in normalized_cols:\n",
    "                print(\n",
    "                    f\"[ERROR] {row['filename']}:{sheet_name} - ID column '{id_column}' not found. Available columns: {df.columns.tolist()}\")\n",
    "                continue\n",
    "            id_column = normalized_cols[id_column.strip()]\n",
    "\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.dropna(subset=[id_column])\n",
    "\n",
    "        full_dupes = df.duplicated()\n",
    "        if full_dupes.any():\n",
    "            print(f\"[QC] {row['filename']}:{sheet_name} - Removed {full_dupes.sum()} fully duplicated rows\")\n",
    "            df = df[~full_dupes]\n",
    "\n",
    "        duplicated_mask = df[id_column].duplicated(keep=False)\n",
    "        duplicated_rows = df[duplicated_mask]\n",
    "\n",
    "        if not duplicated_rows.empty:\n",
    "            duplicated_ids = df[duplicated_mask][id_column].unique()\n",
    "            print(\n",
    "                f\"[DUPLICATES] {row['filename']}:{sheet_name} - {len(duplicated_rows)} duplicate rows on '{id_column}' → {list(duplicated_ids)}\")\n",
    "            df = df[~duplicated_mask]\n",
    "        else:\n",
    "            print(f\"[DUPLICATES] {row['filename']}:{sheet_name} - No duplicates in '{id_column}'\")\n",
    "\n",
    "        melted = df.melt(id_vars=[id_column], var_name=\"predicate\", value_name=\"object_value\")\n",
    "        melted = melted.rename(columns={id_column: \"subject\"})\n",
    "        melted = melted.dropna(subset=[\"subject\", \"predicate\", \"object_value\"])\n",
    "        melted[\"subject\"] = melted[\"subject\"].astype(str).apply(\n",
    "            lambda x: f\"{n4l_prefix}{safe_iri_component(x.strip())}\")\n",
    "        melted[\"predicate\"] = melted[\"predicate\"].astype(str).apply(\n",
    "            lambda x: f\"{n4l_prefix}{safe_iri_component(x.strip())}\")\n",
    "        melted[\"source_file\"] = row[\"filename\"]\n",
    "        melted[\"source_sheet\"] = sheet_name\n",
    "        melted[\"graph\"] = graph_iri\n",
    "        melted_frames.append(melted)\n",
    "        print(f\"[INFO] {row['filename']}:{sheet_name} → {melted.shape[0]} melted rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed processing {row['filename']}:{sheet_name} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcaa4e40f87f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all into one frame\n",
    "combined_df = pd.concat(melted_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00521709df88e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03bcd1f929b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015cf0a15db9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745e6a9bc2011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55e4c17990c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(subset=[\"subject\", \"predicate\", \"object_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31b34d42367c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099e6af0646839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0d873dbbdf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99652621c9fd1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795446771023b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0be9c86f165355",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_mapping = dict(zip(mapping_df[\"original_predicate\"], mapping_df[\"normalized_predicate\"], strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9726367c8a41859",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in combined_df.iterrows():\n",
    "    subj = URIRef(row[\"subject\"])\n",
    "\n",
    "    if row[\"predicate\"] not in predicate_mapping:\n",
    "        raise ValueError(f\"Predicate not found in mapping: {row['predicate']}\")\n",
    "\n",
    "    pred_iri = predicate_mapping[row[\"predicate\"]]\n",
    "    pred = URIRef(pred_iri)\n",
    "\n",
    "    obj = safe_object_term(row[\"object_value\"])\n",
    "    graph_iri = URIRef(row[\"graph\"])\n",
    "\n",
    "    ds.add((subj, pred, obj, graph_iri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78224c4c47a8cc",
   "metadata": {},
   "source": "11 min"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdc65a2fff4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Serialize to N-Quads file\n",
    "ds.serialize(destination=nq_out, format=\"nquads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6424521998905",
   "metadata": {},
   "source": "now zip and load into graphdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
