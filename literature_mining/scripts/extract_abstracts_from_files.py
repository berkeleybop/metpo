#!/usr/bin/env python3
"""
Extract abstracts from PDF and markdown files for papers that couldn't be fetched via API.

This script processes the 6 papers from the failed retrieval list and extracts
their abstracts from local PDF/MD files.
"""

import re
from pathlib import Path
from typing import Optional, Dict, List


def extract_abstract_from_markdown(md_path: Path) -> Optional[str]:
    """
    Extract abstract from a markdown file.

    Looks for patterns like:
    - "Abstract" or "ABSTRACT" as a standalone header
    - Text following that header until the next major section

    Args:
        md_path: Path to the markdown file

    Returns:
        Abstract text, or None if not found
    """
    with open(md_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Try to find abstract section
    # Pattern 1: Look for "Abstract" header followed by content
    patterns = [
        # Page-based format (like MDPI papers) - stop at Keywords or numbered sections
        r'(?:^|\n)## Page \d+\s*\n+[Aa]bstract\s*\n+(.*?)(?=Keywords:|[12]\.\s+[A-Z]|\n#{1,3}\s+[A-Z]|\Z)',
        # Markdown header format
        r'(?:^|\n)#{1,3}\s*[Aa]bstract\s*\n+(.*?)(?=\n#{1,3}\s+\w|Keywords:|\Z)',
        # Plain text header
        r'(?:^|\n)[Aa]bstract\s*\n+(.*?)(?=\nKeywords:|\n[A-Z][a-z]+\s*\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, content, re.DOTALL | re.MULTILINE)
        if match:
            abstract = match.group(1).strip()
            # Clean up the abstract
            abstract = re.sub(r'\n+', ' ', abstract)  # Replace newlines with spaces
            abstract = re.sub(r'\s+', ' ', abstract)  # Normalize whitespace
            abstract = abstract.strip()

            # Remove common artifacts
            abstract = re.sub(r'^\d+â†’', '', abstract)  # Remove line numbers

            if len(abstract) > 100:  # Sanity check - abstract should be substantial
                return abstract

    return None


def extract_doi_from_filename(filename: str) -> Optional[str]:
    """
    Extract DOI from filename.

    Examples:
        doi_10_3390-su151814000.pdf -> 10.3390/su151814000
        doi_10_1038-nchembio_1947.md -> 10.1038/nchembio.1947
    """
    if not filename.startswith('doi_'):
        return None

    # Remove doi_ prefix and file extension
    doi_part = filename[4:]  # Remove "doi_"
    doi_part = doi_part.rsplit('.', 1)[0]  # Remove extension

    # Convert underscores and hyphens back to DOI format
    # First underscore after doi_ becomes the first dot
    # Subsequent hyphens become dots, underscores become slashes or dots
    parts = doi_part.split('_', 1)  # Split on first underscore
    if len(parts) != 2:
        return None

    prefix = parts[0]  # e.g., "10"
    suffix = parts[1]   # e.g., "3390-su151814000"

    # Replace hyphens with appropriate characters
    # Common pattern: publisher-journal_year_issue
    suffix = suffix.replace('-', '.').replace('_', '.')

    doi = f"{prefix}.{suffix}"

    # Handle special cases
    doi = doi.replace('..', '.')  # Remove double dots

    return doi


def sanitize_doi_for_filename(doi: str) -> str:
    """
    Convert DOI to the filename format used in the publications directory.

    Examples:
        10.1016/j.seppur.2025.131701 -> 10_1016-j_seppur_2025_131701
        10.3390/su151814000 -> 10_3390-su151814000
        10.1038/nchembio.1947 -> 10_1038-nchembio_1947
    """
    # Replace the first dot (in 10.xxxx) with underscore
    # Replace forward slashes with hyphens
    # Replace remaining dots with underscores
    result = doi.replace('.', '_', 1)  # First dot becomes underscore
    result = result.replace('/', '-')   # Slashes become hyphens
    result = result.replace('.', '_')   # Remaining dots become underscores
    return result


def extract_metadata_from_markdown(md_path: Path) -> Dict[str, str]:
    """
    Extract metadata (title, authors, journal) from markdown if available.

    Args:
        md_path: Path to markdown file

    Returns:
        Dictionary with metadata fields
    """
    metadata = {
        'title': '',
        'authors': '',
        'journal': '',
        'year': ''
    }

    # For now, return empty metadata - can be enhanced if needed
    # The markdown files from PDF conversion may not have structured metadata

    return metadata


def process_failed_papers():
    """
    Process the 6 papers that failed API retrieval.

    Based on the abstract_fetching_report.md, these are:
    1. 10.1016/j.seppur.2025.131701 (CMM-AI)
    2. 10.1038/nchembio.1947 (CMM-AI)
    3. 10.1101/2023.09.15.557123 (CMM-AI)
    4. 10.1007/s11756-024-01654-0 (PFAS-AI)
    5. 10.1134/S0003683817050027 (PFAS-AI)
    6. 10.3390/su151814000 (PFAS-AI)
    """

    # DOI to directory mapping
    papers = [
        ('10.1016/j.seppur.2025.131701', 'literature_mining/CMM-AI/publications'),
        ('10.1038/nchembio.1947', 'literature_mining/CMM-AI/publications'),
        ('10.1101/2023.09.15.557123', 'literature_mining/CMM-AI/publications'),
        ('10.1007/s11756-024-01654-0', 'literature_mining/PFAS-AI/publications'),
        ('10.1134/S0003683817050027', 'literature_mining/PFAS-AI/publications'),
        ('10.3390/su151814000', 'literature_mining/PFAS-AI/publications'),
    ]

    output_dir = Path('literature_mining/abstracts')
    output_dir.mkdir(parents=True, exist_ok=True)

    results = {
        'success': [],
        'failed': []
    }

    for doi, pub_dir in papers:
        print(f"\nProcessing: {doi}")

        # Find the markdown file
        pub_path = Path(pub_dir)
        safe_doi = sanitize_doi_for_filename(doi)
        md_file = pub_path / f"doi_{safe_doi}.md"

        # Handle special versioning (bioRxiv)
        if not md_file.exists():
            # Try with version suffix
            md_file = pub_path / f"doi_{safe_doi}v1.md"

        if not md_file.exists():
            print(f"  âœ— Markdown file not found: {md_file}")
            results['failed'].append(doi)
            continue

        # Extract abstract
        abstract = extract_abstract_from_markdown(md_file)

        if not abstract:
            print(f"  âœ— Could not extract abstract from {md_file.name}")
            results['failed'].append(doi)
            continue

        print(f"  âœ“ Extracted abstract ({len(abstract)} chars)")

        # Save to file
        output_file = output_dir / f"doi_{safe_doi}-abstract.txt"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"Title: [Extracted from local file]\n\n")
            f.write(f"Authors: [See PDF for full details]\n\n")
            f.write(f"Journal: [See PDF for full details]\n\n")
            f.write(f"DOI: {doi}\n")
            f.write(f"Source: Local file extraction\n")
            f.write(f"\nAbstract:\n{abstract}\n")

        print(f"  ðŸ’¾ Saved to: {output_file.name}")
        results['success'].append(doi)

    # Summary
    print("\n" + "=" * 60)
    print("Summary:")
    print(f"  âœ“ Successfully extracted: {len(results['success'])}")
    print(f"  âœ— Failed: {len(results['failed'])}")

    if results['success']:
        print("\nSuccessfully processed:")
        for doi in results['success']:
            print(f"  â€¢ {doi}")

    if results['failed']:
        print("\nFailed to process:")
        for doi in results['failed']:
            print(f"  â€¢ {doi}")

    return results


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Extract abstracts from local PDF/MD files for failed API retrievals"
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help="Show what would be extracted without saving files"
    )

    args = parser.parse_args()

    if args.dry_run:
        print("DRY RUN MODE - No files will be saved\n")

    results = process_failed_papers()

    return 0 if not results['failed'] else 1


if __name__ == '__main__':
    exit(main())
